# -*- coding: utf-8 -*-
"""SmartSDLC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1S6AauZswyi-XcSYXIQQoQ3aAiEL8r_sp
"""

!pip install --upgrade pip
!pip install transformers accelerate bitsandbytes pymupdf

from huggingface_hub import login

# Paste your token here
login("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")

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_name = "ibm-granite/granite-3.3-2b-instruct"

tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype=torch.float16
)

import fitz  # PyMuPDF

def extract_text_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    text = "\n".join([page.get_text() for page in doc])
    return text

def classify_requirement_from_pdf(file):
    text = extract_text_from_pdf(file.name)
    prompt = f"""Classify each line of the following software requirement text into one of the SDLC phases: Requirements, Design, Development, Testing, Deployment.

Text:
{text}

Return the output as JSON in the format:
[{{"sentence": "...", "phase": "..."}}]
"""
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    output = model.generate(**inputs, max_new_tokens=1024)
    return tokenizer.decode(output[0], skip_special_tokens=True)

def generate_code(prompt):
    final_prompt = f"Generate clean, readable code for the following requirement:\n{prompt}"
    inputs = tokenizer(final_prompt, return_tensors="pt").to("cuda")
    output = model.generate(**inputs, max_new_tokens=512)
    return tokenizer.decode(output[0], skip_special_tokens=True)

def fix_code_bug(buggy_code):
    prompt = f"Fix bugs in the following code:\n{buggy_code}\nReturn only the corrected version."
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    output = model.generate(**inputs, max_new_tokens=512)
    return tokenizer.decode(output[0], skip_special_tokens=True)

def generate_test_cases(function_code):
    prompt = f"Write unit tests using unittest framework for the following Python function:\n{function_code}"
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    output = model.generate(**inputs, max_new_tokens=512)
    return tokenizer.decode(output[0], skip_special_tokens=True)

def summarize_code(code_snippet):
    prompt = f"Summarize the functionality of this code:\n{code_snippet}"
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    output = model.generate(**inputs, max_new_tokens=256)
    return tokenizer.decode(output[0], skip_special_tokens=True)

def chatbot_response(user_input):
    prompt = f"You are a helpful assistant for SDLC-related questions. Answer:\n{user_input}"
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    output = model.generate(**inputs, max_new_tokens=256)
    return tokenizer.decode(output[0], skip_special_tokens=True)

import gradio as gr

with gr.Blocks(title="SmartSDLC AI Platform") as demo:
    gr.Markdown("# ü§ñ SmartSDLC ‚Äì AI-enhanced SDLC Automation Platform")

    with gr.Tab("1Ô∏è‚É£ Requirement Classifier"):
        file_input = gr.File(label="Upload Requirement PDF")
        req_output = gr.Textbox(label="Classified Output")
        file_input.change(classify_requirement_from_pdf, inputs=file_input, outputs=req_output)

    with gr.Tab("2Ô∏è‚É£ Code Generator"):
        prompt_input = gr.Textbox(label="Enter requirement prompt")
        code_output = gr.Code(label="Generated Code", language="python")
        prompt_input.change(generate_code, inputs=prompt_input, outputs=code_output)

    with gr.Tab("3Ô∏è‚É£ Bug Fixer"):
        bug_input = gr.Code(label="Enter Buggy Code", language="python")
        fix_output = gr.Code(label="Fixed Code", language="python")
        bug_input.change(fix_code_bug, inputs=bug_input, outputs=fix_output)

    with gr.Tab("4Ô∏è‚É£ Test Case Generator"):
        test_input = gr.Code(label="Enter Code for Test Case", language="python")
        test_output = gr.Code(label="Generated Unit Tests", language="python")
        test_input.change(generate_test_cases, inputs=test_input, outputs=test_output)

    with gr.Tab("5Ô∏è‚É£ Code Summarizer"):
        sum_input = gr.Code(label="Code to Summarize", language="python")
        sum_output = gr.Textbox(label="Summary")
        sum_input.change(summarize_code, inputs=sum_input, outputs=sum_output)

    with gr.Tab("6Ô∏è‚É£ Chatbot Assistant"):
        chat_input = gr.Textbox(label="Ask anything about SDLC")
        chat_output = gr.Textbox(label="Bot Response")
        chat_input.change(chatbot_response, inputs=chat_input, outputs=chat_output)

demo.launch()